{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÅ Drone Resupply Benchmark - Google Colab\n",
    "\n",
    "Notebook n√†y cho ph√©p b·∫°n ch·∫°y benchmark b√†i to√°n Drone Resupply tr√™n Google Colab.\n",
    "\n",
    "## H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:\n",
    "1. **Upload project**: T·∫£i file ZIP c·ªßa project l√™n Google Drive\n",
    "2. **Ch·∫°y c√°c cell theo th·ª© t·ª±** t·ª´ tr√™n xu·ªëng d∆∞·ªõi\n",
    "3. **K·∫øt qu·∫£**: File CSV v√† solutions s·∫Ω ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload v√† gi·∫£i n√©n Project\n",
    "\n",
    "**C√°ch 1**: N·∫øu b·∫°n ƒë√£ upload file `newsolve.zip` l√™n Drive, ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê∆∞·ªùng d·∫´n t·ªõi file ZIP tr√™n Drive (thay ƒë·ªïi n·∫øu c·∫ßn)\n",
    "ZIP_PATH = '/content/drive/MyDrive/newsolve.zip'\n",
    "\n",
    "# Gi·∫£i n√©n\n",
    "!unzip -o \"{ZIP_PATH}\" -d /content/newsolve\n",
    "print(\"Gi·∫£i n√©n ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C√°ch 2**: Upload tr·ª±c ti·∫øp t·ª´ m√°y t√≠nh (n·∫øu ch∆∞a c√≥ tr√™n Drive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment ƒë·ªÉ upload tr·ª±c ti·∫øp\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Ch·ªçn file newsolve.zip\n",
    "# !unzip -o newsolve.zip -d /content/newsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. C√†i ƒë·∫∑t Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/newsolve\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Th∆∞ m·ª•c hi·ªán t·∫°i:\", os.getcwd())\n",
    "print(\"\\nN·ªôi dung:\")\n",
    "!ls -la\n",
    "print(\"\\nTh∆∞ m·ª•c data:\")\n",
    "!ls data/ 2>/dev/null || echo \"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import c√°c module c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/newsolve/src')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from typing import Dict\n",
    "\n",
    "from models import load_problem\n",
    "from initializer import SolutionInitializer\n",
    "from optimization import TabuSearch\n",
    "\n",
    "print(\"Import th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ƒê·ªãnh nghƒ©a h√†m Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instance(filepath: str, solution_output_dir: str, num_rounds: int, num_iters: int) -> Dict:\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω m·ªôt instance nhi·ªÅu l·∫ßn.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        instance_name = os.path.basename(filepath)\n",
    "        problem = load_problem(filepath)\n",
    "        \n",
    "        makespans = []\n",
    "        runtimes = []\n",
    "        best_sol_object = None\n",
    "        best_makespan_instance = float('inf')\n",
    "        \n",
    "        for r in range(num_rounds):\n",
    "            start_t = time.time()\n",
    "            \n",
    "            # Init\n",
    "            initializer = SolutionInitializer(problem)\n",
    "            init_sol = initializer.initialize4bench()\n",
    "            \n",
    "            # Optimization\n",
    "            tabu = TabuSearch(problem, init_sol)\n",
    "            final_sol = tabu.solve4bench(max_iterations=num_iters, tabu_tenure=20)\n",
    "            \n",
    "            end_t = time.time()\n",
    "            duration = end_t - start_t\n",
    "            \n",
    "            ms = final_sol.calculate_makespan()\n",
    "            feasible, _ = final_sol.is_feasible()\n",
    "            \n",
    "            if not feasible:\n",
    "                ms = float('inf')\n",
    "            \n",
    "            makespans.append(ms)\n",
    "            runtimes.append(duration)\n",
    "            \n",
    "            if ms < best_makespan_instance:\n",
    "                best_makespan_instance = ms\n",
    "                best_sol_object = final_sol.copy()\n",
    "        \n",
    "        avg_makespan = statistics.mean(makespans)\n",
    "        avg_runtime = statistics.mean(runtimes)\n",
    "        \n",
    "        # Save best solution\n",
    "        if best_sol_object:\n",
    "            sol_filename = instance_name.replace(\".txt\", \"_sol.txt\")\n",
    "            sol_path = os.path.join(solution_output_dir, sol_filename)\n",
    "            best_sol_object.save_to_file(sol_path)\n",
    "            \n",
    "        print(f\"‚úì {instance_name:<30} | Best: {best_makespan_instance:<10.2f} | Avg: {avg_makespan:<10.2f} | Time: {avg_runtime:.2f}s\")\n",
    "        \n",
    "        return {\n",
    "            \"Instance\": instance_name,\n",
    "            \"Best Objective\": best_makespan_instance,\n",
    "            \"Avg Objective\": avg_makespan,\n",
    "            \"Avg Time (s)\": avg_runtime\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó ERROR {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_benchmark(data_folder: str, output_csv: str, solution_dir: str, \n",
    "                  num_rounds: int = 10, num_iters: int = 200, max_workers: int = None):\n",
    "    \"\"\"\n",
    "    Ch·∫°y benchmark song song tr√™n t·∫•t c·∫£ instances.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    os.makedirs(solution_dir, exist_ok=True)\n",
    "    \n",
    "    instance_files = sorted(glob.glob(os.path.join(data_folder, \"*.txt\")))\n",
    "    \n",
    "    if not instance_files:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y file .txt trong {data_folder}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu benchmark {len(instance_files)} instances...\")\n",
    "    print(f\"   Rounds: {num_rounds} | Iterations: {num_iters} | Workers: {max_workers or 'Auto'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Parallel execution\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_instance, f, solution_dir, num_rounds, num_iters): f \n",
    "            for f in instance_files\n",
    "        }\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                results.append(res)\n",
    "    \n",
    "    # Sort and create DataFrame\n",
    "    results.sort(key=lambda x: x['Instance'])\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    if not df.empty:\n",
    "        avg_row = {\n",
    "            \"Instance\": \"AVERAGE\",\n",
    "            \"Best Objective\": df[\"Best Objective\"].mean(),\n",
    "            \"Avg Objective\": df[\"Avg Objective\"].mean(),\n",
    "            \"Avg Time (s)\": df[\"Avg Time (s)\"].mean()\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚úÖ Ho√†n t·∫•t! K·∫øt qu·∫£ l∆∞u t·∫°i: {output_csv}\")\n",
    "    print(f\"   Solutions l∆∞u t·∫°i: {solution_dir}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"H√†m benchmark ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ CH·∫†Y BENCHMARK\n",
    "\n",
    "**T√πy ch·ªânh c√°c tham s·ªë b√™n d∆∞·ªõi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== C·∫§U H√åNH ==============\n",
    "DATA_FOLDER = \"./data/0130/10_instances\"    # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu test\n",
    "OUTPUT_CSV = \"./output/benchmark_results.csv\"  # File k·∫øt qu·∫£\n",
    "SOLUTION_DIR = \"./output/solutions\"         # Th∆∞ m·ª•c l∆∞u solutions\n",
    "\n",
    "NUM_ROUNDS = 10     # S·ªë l·∫ßn ch·∫°y m·ªói instance\n",
    "NUM_ITERS = 200     # S·ªë v√≤ng l·∫∑p Tabu Search\n",
    "MAX_WORKERS = 2     # S·ªë workers song song (Colab c√≥ 2 CPU cores)\n",
    "# ======================================\n",
    "\n",
    "# Ch·∫°y benchmark\n",
    "results_df = run_benchmark(\n",
    "    data_folder=DATA_FOLDER,\n",
    "    output_csv=OUTPUT_CSV,\n",
    "    solution_dir=SOLUTION_DIR,\n",
    "    num_rounds=NUM_ROUNDS,\n",
    "    num_iters=NUM_ITERS,\n",
    "    max_workers=MAX_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Xem k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df is not None:\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"Ch∆∞a c√≥ k·∫øt qu·∫£. H√£y ch·∫°y benchmark tr∆∞·ªõc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. T·∫£i k·∫øt qu·∫£ v·ªÅ m√°y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# T·∫£i file CSV\n",
    "files.download(OUTPUT_CSV)\n",
    "\n",
    "# N√©n v√† t·∫£i th∆∞ m·ª•c solutions\n",
    "!zip -r solutions.zip {SOLUTION_DIR}\n",
    "files.download('solutions.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (T√πy ch·ªçn) L∆∞u k·∫øt qu·∫£ l√™n Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy k·∫øt qu·∫£ l√™n Drive\n",
    "DRIVE_OUTPUT = \"/content/drive/MyDrive/benchmark_results\"\n",
    "!mkdir -p \"{DRIVE_OUTPUT}\"\n",
    "!cp {OUTPUT_CSV} \"{DRIVE_OUTPUT}/\"\n",
    "!cp -r {SOLUTION_DIR} \"{DRIVE_OUTPUT}/\"\n",
    "print(f\"ƒê√£ l∆∞u k·∫øt qu·∫£ l√™n Drive: {DRIVE_OUTPUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
